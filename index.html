<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!-- <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script> -->
    <!-- Enable MathJax for LaTeX rendering -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        packages: ['base', 'ams']
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#AUTHOR-1" class="author-link placeholder-link">Wenxuan Song<sup>1*</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="#AUTHOR-2" class="author-link placeholder-link">Jiayi Chen<sup>1*</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="#AUTHOR-3" class="author-link placeholder-link">Pengxiang Ding<sup>2,3†</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="#AUTHOR-4" class="author-link placeholder-link">Yuxin Huang<sup>1</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="#AUTHOR-5" class="author-link placeholder-link">Han Zhao<sup>2,3</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="#AUTHOR-6" class="author-link placeholder-link">Donglin Wang<sup>2</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block corresponding-author">
                <a href="#CORRESPONDING-AUTHOR" class="author-link placeholder-link">Haoang Li<sup>1‡</sup></a>
              </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </div>
            <div class="is-size-5 publication-affiliations">
              <span class="affiliation-block">
                <sup>1</sup>IRPN Lab, HKUST(GZ)<br>
                <sup>2</sup>MiLab, Westlake University<br>
                <sup>3</sup>Zhejiang University
               </span>
               <div class="contribution-notes">
                <small>
                  <sup>*</sup>Equal Contribution &nbsp;
                  <sup>†</sup>Joint Second Author &nbsp;
                  <sup>‡</sup>Corresponding Author
                </small>
              </div>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2506.13725" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>



                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/OpenHelix-Team/CEED-VLA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.13725" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, Vision-Language-Action (VLA) models have become a vital research direction in robotics due to their impressive multimodal understanding and generalization capabilities. Despite the progress, their practical deployment is severely constrained by inference speed bottlenecks, particularly in high-frequency and dexterous manipulation tasks.
            While recent studies have explored Jacobi decoding as a more efficient alternative to traditional autoregressive decoding, its practical benefits are marginal due to the lengthy iterations.
            To address it, we introduce consistency distillation training to predict multiple correct action tokens in a single iteration, thereby achieving acceleration.
            Besides, we design mixed-label supervision to mitigate the error accumulation during distillation.
            Although distillation brings acceptable speedup, we identify that certain inefficient iterations remain a critical bottleneck. To tackle this, we propose an early-exit decoding strategy that moderately relaxes convergence conditions, which further improves average inference efficiency.
            Experimental results show that the proposed method achieves 4× inference acceleration across different baselines while maintaining high task success rates in both simulated and real-world robot tasks. 
            These experiments validate that our approach provides an efficient and general paradigm for accelerating multimodal decision-making in robotics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Introduce</h2>

    <div class="box" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/small_teaser.png" target="_blank">
        <img src="paper_image/small_teaser.png" alt="CEED-VLA Model Architecture"
             style="max-width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto;">
      </a>

      <ul style="font-size: 0.95rem; margin-top: 1rem; padding-left: 1.2em;">
        <li>(1) We propose <strong>CEED-VLA</strong>, a universal acceleration method that achieves significant inference speedup while maintaining manipulation performance.</li>
        <li>(2) We introduce a <strong>consistency distillation</strong> process and a <strong>mixed-label supervision</strong> in the autoregressive loss to retain high-quality actions.</li>
        <li>(3) We identify <strong>inefficient iteration</strong> as the bottleneck of Jacobi decoding and propose an <strong>early-exit decoding</strong> strategy, leading to <span style="font-family: 'Times New Roman', serif;">4.1×</span> speedup and over <span style="font-family: 'Times New Roman', serif;">4.3×</span> decoding frequency.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>

    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/teaser.jpg" target="_blank">
        <img src="paper_image/teaser.jpg" alt="CEED-VLA Model Architecture"
             style="max-width: 100%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The architecture of CEED-VLA with consistency distillation and early-exit decoding.</p>
    </div>

    <!-- Jacobi Trajectory Collection Section -->
    <h3 class="title is-4 mt-6">Jacobi Trajectory Collection</h3>
    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/alogorithm2.png"" target="_blank">
        <img src="paper_image/alogorithm2.png" alt="CEED-VLA Model Architecture"
             style="max-width: 80%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The Jacobi trajectory collection process.</p> 
    </div>
    <p style="font-size: 0.95rem; margin-top: 1rem;">
      For the target VLA \( P \), we let \( Q_\theta(\cdot \mid \mathbf{x}) \) denote the CEED-VLA model 
      with parameters \( \theta \) initialized from \( P \). To capture the inherent consistency within Jacobi trajectories, 
      we first collect them by prompting \( P \) to predict actions using Jacobi decoding on the robot dataset \( \mathcal{C} \).
    </p>

    <!-- Loss Design Explanation -->
    <div class="content mt-6" style="font-size: 0.95rem;">
      <h3 class="title is-4">Consistency Training</h3>
      <p style="margin-top: -0.5rem; font-size: 0.95rem;">
        The consistency training procedure optimizes two objectives: 
        <em>(i)</em> guiding the model to predict multiple correct tokens simultaneously, and 
        <em>(ii)</em> constraining CEED-VLA from drifting away from the target VLA distribution to preserve manipulation skills.
      </p>
      <h4><strong>1. Consistency Loss</strong></h4>
      <p>
        We design a consistency loss to ensure that any intermediate point \( \mathcal{Y} \) on the Jacobi trajectory \( \mathcal{J} \) is mapped to the fixed point \( \mathcal{Y}^* \). This encourages the model to converge efficiently.
      </p>
      <p>
        \[
        \mathcal{L}_{\text{C}} = \mathbb{E}_{(\mathbf{x}, \mathcal{J}) \sim \mathcal{D}, \mathcal{Y} \sim \mathcal{J}} \left[ \sum_{i=1}^n \text{KL}\left( Q_{\theta^{-}}(\cdot|\mathcal{Y}_i^*, \mathbf{x}) \,\|\, Q_{\theta}(\cdot|\mathcal{Y}_i, \mathbf{x}) \right) \right]
        \]
      </p>
      <p>
        where \( \theta^{-} = \text{stopgrad}(\theta) \) and KL denotes the forward KL divergence.
      </p>

      <h4 class="mt-5"><strong>2. Mixed-label AR Supervision</strong></h4>
      <p>
        To maintain autoregressive behavior, we retain the AR loss from the teacher model:
      </p>
      
      <p>
        $$
        \mathcal{L}_{\text{AR}} = \mathbb{E}_{(\mathbf{x}, \mathcal{Y}^{*}) \sim \mathcal{D}} \left[ -\sum_{i=1}^N \log Q_{\theta}\left( \mathcal{Y}^{*}_{i} \mid \mathcal{Y}^{*}_{\lt i}, \mathbf{x} \right) \right]
        $$
        
      </p>
      
      <p>
        For outliers with large deviation (measured by L1 distance), the AR labels are replaced by corresponding ground-truth values.
      </p>

      <h4 class="mt-5"><strong>3. Total Loss</strong></h4>
      <p>
        \[
        \mathcal{L}(\theta) = \mathcal{L}_{\text{C}} + \omega \cdot \mathcal{L}_{\text{AR}}
        \]
      </p>
      <p>
        where \( \omega \) is a weighting coefficient balancing consistency and AR supervision.
      </p>
    </div>

    <!-- Early-Exit Decoding Section -->
    <h3 class="title is-4 mt-6">Early-Exit Decoding</h3>
    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/four decode compare.png"" target="_blank">
        <img src="paper_image/four decode compare.png" alt="CEED-VLA Model Architecture"
             style="max-width: 80%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The comparison of four decoding methods.</p> 
    </div>
    <p style="font-size: 0.95rem; margin-top: 1rem;">
      To address the inefficiency of Jacobi decoding caused by strict convergence requirements, we propose early-exit decoding.
      Instead of waiting for full convergence,the model exits early at a predefined step and directly outputs the intermediate result. 
      This strategy leverages the observation that late-stage token updates are often minor and have limited impact on task success. 
      By relaxing the stopping condition, early-exit decoding significantly improves both the average and minimum inference speeds while preserving manipulation accuracy, enabling high-frequency robotic control with minimal delay.
      
    </p>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiment</h2>

    <div class="box" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/table2.png" target="_blank">
        <img src="paper_image/table2.png" alt="CEED-VLA Model Architecture"
             style="max-width: 100%; height: auto; border-radius: 8px; display: block; margin: 0 auto;">
      </a>
      <p class="subtitle is-5 mt-3 has-text-centered">Main experiment.</p>
    </div>
    <h3 class="title is-4 mt-6">Evaluation on CALVIN ABC->D Benchmark</h3>
    <div class="columns is-multiline is-mobile is-centered mt-4">
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/video0.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 1: rotate_blue_block_right</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/video1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 2: move_slider_right</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/video2.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 3: lift_red_block_slider</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/video3.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 4:<br>place_in_slider</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/video4.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 5: turn_off_lightbulb</p>
      </div>
    </div>
    <!-- <div class="box mt-6" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;">
      <video autoplay muted loop playsinline style="max-width: 50%; border-radius: 8px;">
        <source src="paper_video/all_x264.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <p class="subtitle is-5 mt-3">Overview of all tasks performed sequentially.</p>
    </div> -->

    <div class="columns is-centered mt-5">
      <div class="column is-half has-text-centered">
        <img src="paper_image/table1.png" alt="Image 1" style="width: 100%; border-radius: 8px;">
        <!-- <p class="mt-2">Table 1: Description</p> -->
      </div>
      <div class="column is-half has-text-centered">
        <img src="paper_image/table4.png" alt="Image 2" style="width: 100%; border-radius: 8px;">
        <!-- <p class="mt-2">Table 4: Description</p> -->
      </div>
    </div>
    
    <div class="columns is-centered mt-3">
      <div class="column is-half has-text-centered">
        <img src="paper_image/table5.png" alt="Image 3" style="width: 100%; border-radius: 8px;">
        <!-- <p class="mt-2">Table 5: Description</p> -->
      </div>
      <div class="column is-half has-text-centered">
        <img src="paper_image/table6.png" alt="Image 4" style="width: 100%; border-radius: 8px;">
        <!-- <p class="mt-2">Table 6: Description</p> -->
      </div>
    </div>



    <h3 class="title is-4 mt-6">Evaluation on LIBERO LONG Benchmark</h3>
    <div class="columns is-multiline is-mobile is-centered mt-4">
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/pick up the book and place it in the back compartment of the caddy.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 1: pick up the book and place it in the back compartment of the caddy</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put both moka pots on the stove.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 2: put both moka pots on the stove</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put both the alphabet soup and the cream cheese box in the basket.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 3: put both the alphabet soup and the cream cheese box in the basket</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put both the alphabet soup and the tomato sauce in the basket.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 4: put both the alphabet soup and the tomato sauce in the basket</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put both the cream cheese box and the butter in the basket.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 5: put both the cream cheese box and the butter in the basket</p>
      </div>
    </div>

    <div class="columns is-multiline is-mobile is-centered mt-4">
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put the black bowl in the bottom drawer of the cabinet and close it.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 6: put the black bowl in the bottom drawer of the cabinet and close it</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put the white mug on the left plate and put the yellow and white mug on the right plate.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 7: put the white mug on the left plate and put the yellow and white mug on the right plate</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put the white mug on the plate and put the chocolate pudding to the right of the plate.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 8: put the white mug on the plate and put the chocolate pudding to the right of the plate</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/put the yellow and white mug in the microwave and close it.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 9: put the white mug on the plate and put the chocolate pudding to the right of the plate</p>
      </div>
      <div class="column is-one-fifth has-text-centered">
        <video autoplay muted loop playsinline width="100%">
          <source src="paper_video/turn_on_the_stove_and_put_the_moka_pot_on_it.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p class="mt-2">Task 10: turn on the stove and put the moka pot on it</p>
      </div>
    </div>
    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/table3.png"" target="_blank">
        <img src="paper_image/table3.png" alt="CEED-VLA Model Architecture"
             style="max-width: 80%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The ablation of CEED-VLA base on Open-VLA.</p> 
    </div>
    
    <h3 class="title is-4 mt-6">Evaluation on Real World </h3>


    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/real_setup.png" target="_blank">
        <img src="paper_image/real_setup.png" alt="CEED-VLA Model Architecture"
             style="max-width: 100%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The real world setup.</p>
    </div>
    <div class="box mt-6" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;">
      <video controls autoplay muted loop playsinline style="max-width: 100%; border-radius: 8px;">
        <source src="paper_video/real_x264.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <!-- <p class="subtitle is-5 mt-3">The real world experiment.</p> -->
      <p class="subtitle is-6 mt-3" style="text-align: left;">
        <b>Task:</b> Fold the towel<br><br>
        The top two videos demonstrate the performance of the <b>LLaVA-VLA</b> model in a real-world setting. The robotic arm operates at a relatively low frequency and struggles with dexterous manipulation tasks such as towel folding, often failing to grasp the towel properly or only grabbing one edge, which leads to task failure.<br><br>
        The bottom two videos show the experimental results of our proposed <b>CEED-VLA</b> model. Thanks to its faster inference speed, the robotic arm operates more frequently and successfully completes the dexterous manipulation tasks.
      </p>
    </div>
    <div class="box mt-6" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;">
      <video controls autoplay muted loop playsinline style="max-width: 100%; border-radius: 8px;">
        <source src="paper_video/color_generation_x264.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <!-- <p class="subtitle is-5 mt-3">The real world experiment.</p> -->
      <p class="subtitle is-5 mt-3">Visual generalization. (Fold the towel with unseen color)</p>
    </div>



    <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
      <a href="paper_image/table7.png"" target="_blank">
        <img src="paper_image/table7.png" alt="CEED-VLA Model Architecture"
             style="max-width: 80%; height: auto; border-radius: 8px;">
      </a>
      <p class="subtitle is-5 mt-3">The real world results.</p> 
    </div>
    

  </div>
</section>






<!-- <section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3 has-text-centered">Experiment</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
          <img src="paper_image/table2.png" alt="Second table"
               style="height: 400px; width: auto; display: block; margin: 0 auto;">
          <h2 class="subtitle has-text-centered">Main Experiment</h2>
        </div>

        <div class="item">
          <img src="paper_image/table4.png" alt="Third table"
               style="height: 400px; width: auto; display: block; margin: 0 auto;">
          <h2 class="subtitle has-text-centered">Main Ablation Experiment</h2>
        </div>

        <div class="item">
          <img src="paper_image/table8.png" alt="Fourth table"
               style="height: 400px; width: auto; display: block; margin: 0 auto;">
          <h2 class="subtitle has-text-centered">The Benefits of Consistency Training.</h2>
        </div>

      </div>
      <h3 class="title is-4 mt-6">Early-Exit Decoding</h3>
      <div class="box has-text-centered" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
        <a href="paper_image/four decode compare.png"" target="_blank">
          <img src="paper_image/four decode compare.png" alt="CEED-VLA Model Architecture"
               style="max-width: 80%; height: auto; border-radius: 8px;">
        </a>
        <p class="subtitle is-5 mt-3">The comparison of four decoding methods.</p> 
      </div>
      <p style="font-size: 0.95rem; margin-top: 1rem;">
        To address the inefficiency of Jacobi decoding caused by strict convergence requirements, we propose early-exit decoding.
        Instead of waiting for full convergence,the model exits early at a predefined step and directly outputs the intermediate result. 
        This strategy leverages the observation that late-stage token updates are often minor and have limited impact on task success. 
        By relaxing the stopping condition, early-exit decoding significantly improves both the average and minimum inference speeds while preserving manipulation accuracy, enabling high-frequency robotic control with minimal delay.
        
      </p>
    </div> -->

    

<!-- </section> -->


<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
    
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
          
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{song2025ceedvla,
      title={CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding}, 
      author={Song, Wenxuan and Chen,  Jiayi  and Ding, Pengxiang and Huang, Yuxin and Zhao, Han and Wang, Donglin and Li, Haoang  and others},
      journal={arXiv preprint arXiv:2506.13725},
      year={2025}
      }  
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
